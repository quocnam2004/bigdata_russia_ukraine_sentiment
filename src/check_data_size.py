# -*- coding: utf-8 -*-
import sys
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from pyspark.sql import SparkSession

# ============================================================
# HÃ€M TÃNH DUNG LÆ¯á»¢NG HDFS (RECURSIVE)
# ============================================================
def get_hdfs_size(fs, Path, path_str, suffix_filter=None):
    """
    Tráº£ vá»:
        total_size (bytes), file_count
    """
    total_size = 0
    file_count = 0

    try:
        if not fs.exists(Path(path_str)):
            return 0, 0

        # True = recursive (quÃ©t cáº£ thÆ° má»¥c con)
        iterator = fs.listFiles(Path(path_str), True)
        
        while iterator.hasNext():
            status = iterator.next()
            path = status.getPath().toString()
            size = status.getLen()
            
            # Bá» qua cÃ¡c file há»‡ thá»‘ng cá»§a Spark/HDFS
            if "_SUCCESS" in path or path.endswith(".crc"):
                continue

            # Logic lá»c Ä‘uÃ´i file (linh hoáº¡t hÆ¡n)
            if suffix_filter:
                # Náº¿u suffix_filter lÃ  list/tuple (vÃ­ dá»¥: ['.gz', '.gzip'])
                if isinstance(suffix_filter, (list, tuple)):
                    if not any(path.endswith(s) for s in suffix_filter):
                        continue
                # Náº¿u lÃ  string Ä‘Æ¡n
                elif not path.endswith(suffix_filter):
                    continue

            total_size += size
            file_count += 1
    except Exception as e:
        print(f"Lá»—i khi quÃ©t Ä‘Æ°á»ng dáº«n {path_str}: {e}")
        return 0, 0

    return total_size, file_count


# ============================================================
# MAIN
# ============================================================
if __name__ == "__main__":

    # --- Sá»¬A Lá»–I NAMENODE: THÃŠM Cáº¤U HÃŒNH EVENT LOG ---
    spark = SparkSession.builder \
        .appName("Check_Data_Size_Final") \
        .master("local[2]") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
        .config("spark.eventLog.enabled", "true") \
        .config("spark.eventLog.dir", "hdfs://localhost:9000/spark-logs") \
        .config("spark.history.fs.logDirectory", "hdfs://localhost:9000/spark-logs") \
        .getOrCreate()

    sc = spark.sparkContext

    # Láº¥y Ä‘á»‘i tÆ°á»£ng FileSystem tá»« JVM
    fs = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(
        sc._jsc.hadoopConfiguration()
    )
    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path

    # --------------------------------------------------------
    # Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
    # --------------------------------------------------------
    RAW_PATH = "/data/raw/"
    PROCESSED_PATH = "/data/processed/clean_tweets"

    print("\n>>> ÄANG KIá»‚M TRA DUNG LÆ¯á»¢NG Dá»® LIá»†U <<<\n")

    # --------------------------------------------------------
    # 1. Dá»® LIá»†U TRÆ¯á»šC ETL (RAW)
    # --------------------------------------------------------
    # Cháº¥p nháº­n cáº£ .gzip vÃ  .gz cho an toÃ n
    raw_size, raw_files = get_hdfs_size(
        fs, Path, RAW_PATH, suffix_filter=[".gzip", ".gz"]
    )

    # --------------------------------------------------------
    # 2. Dá»® LIá»†U SAU ETL (CLEAN)
    # --------------------------------------------------------
    processed_size, processed_files = get_hdfs_size(
        fs, Path, PROCESSED_PATH
    )

    # --------------------------------------------------------
    # IN Káº¾T QUáº¢
    # --------------------------------------------------------
    GB = 1024 ** 3
    MB = 1024 ** 2

    print("=" * 70)
    print("DUNG LÆ¯á»¢NG Dá»® LIá»†U TRÆ¯á»šC & SAU PREPROCESS")
    print("=" * 70)

    print(f"ğŸ“¥ RAW DATA")
    print(f"   â€¢ Sá»‘ file       : {raw_files:,}")
    print(f"   â€¢ Tá»•ng dung lÆ°á»£ng: {raw_size / GB:,.2f} GB ({raw_size / MB:,.2f} MB)")

    print("-" * 70)

    print(f"ğŸ“¤ PROCESSED DATA (Parquet - Cleaned)")
    print(f"   â€¢ Sá»‘ file       : {processed_files:,}")
    print(f"   â€¢ Tá»•ng dung lÆ°á»£ng: {processed_size / GB:,.2f} GB ({processed_size / MB:,.2f} MB)")

    print("-" * 70)

    if raw_size > 0:
        ratio = processed_size / raw_size * 100
        print(f"ğŸ“Š Tá»¶ Lá»† Dá»® LIá»†U CÃ’N Láº I: {ratio:.2f}%")
        print(f"ğŸ“‰ ÄÃƒ GIáº¢M (Lá»c rÃ¡c + NÃ©n): {100 - ratio:.2f}%")
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u Raw (hoáº·c sai Ä‘uÃ´i file).")

    print("=" * 70)

    spark.stop()